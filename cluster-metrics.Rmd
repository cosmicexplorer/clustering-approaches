---
title: "Cluster Metrics"
author: Danny McClanahan
date: January 6, 2017
---

# Precision and Recall

## Homogeneity

- How similar is each element in a given cluster?
    - How well does the cluster predict the elements within it?

## Specificity

- How different is each cluster from the others?
    - How significant is the difference / how reliably do we get a significant difference across one or more features between two elements in separate clusters?

## Examples

1. Jaccard Similarity
2. F-measure
3. Pearson corr/cross prods
    - gives direction/strength of association between two datasets
    - a *low* strength means a *more* specific (and therefore a "better" clustering)

# Descriptivity

- How accurately does the clustering model the sample population?
    - How likely is the clustering's assumed generative model (usually a GMM) to have produced the output?
    - Can we characterize which instances the clustering models the sample well and which it does not?

## Examples

1. Kolmogorov-Smirnov (K-S)
2. **(?)** Fisher information

# Compression

- How well does the clustering compress the dataset?
- How complex is the data within a single cluster?
    - How much information does each cluster contain about the rest of the data (or vice versa)?

## Examples

1. Shannon Entropy Estimation
2. Minimum Description Length (MDL)
3. (Estimation of) Kolmogorov Complexity / Entropy
4. Absolute Mutual Information

# Citations
- https://arxiv.org/pdf/cs/0404039v1.pdf
- https://en.wikipedia.org/wiki/Dunn_index
- https://en.wikipedia.org/wiki/Silhouette_(clustering)
- https://en.wikipedia.org/wiki/Cophenetic_correlation
- https://en.wikipedia.org/wiki/Rand_index
- http://homepages.cwi.nl/~paulv/papers/cluster.pdf
- https://en.wikipedia.org/wiki/Kolmogorov_complexity#Uncomputability_of_Kolmogorov_complexity
- https://en.wikipedia.org/wiki/Mutual_information

# BibTeX Citations
@Article{Hubert1972,
author="Hubert, Lawrence",
title="Some extensions of Johnson's hierarchical clustering algorithms",
journal="Psychometrika",
year="1972",
volume="37",
number="3",
pages="261--274",
abstract="Considerable attention has been given in the psychological literature to techniques of data reduction that partition a set of objects into optimally homogeneous groups. This paper is an attempt to extend the hierarchical partitioning algorithms proposed by Johnson and to emphasize a general connection between these clustering procedures and the mathematical theory of lattices. A goodness-of-fit statistic is first proposed that is invariant under monotone increasing transformations of the basic similarity matrix. This statistic is then applied to three illustrative hierarchical clusterings: two obtained by the Johnson algorithms and one obtained by an algorithm that produces the same chain under hypermonotone increasing transformations of the similarity measures.",
issn="1860-0980",
doi="10.1007/BF02306783",
url="http://dx.doi.org/10.1007/BF02306783"
}
